{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIFE CYCLE OF A DATA SCIENTIS PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA GATHERING\n",
    "\n",
    "-> web scrapping\n",
    "\n",
    "-> 3th party apis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIG DATA ENGINEERING. -> Data Clustring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING\n",
    "\n",
    "-> handle the null/missing values\n",
    "\n",
    "-> handle inbalance in datasets\n",
    "\n",
    "-> try to remove noise from data\n",
    "\n",
    "-> formate the data in proper way\n",
    "\n",
    "-> clean the data\n",
    "\n",
    "-> normalization\n",
    "\n",
    "-> handle categarical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SELECTION\n",
    "\n",
    "-> pearson correlation\n",
    "\n",
    "-> heatmap\n",
    "\n",
    "-> extra tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL CREATION\n",
    "\n",
    "-> select the right suitable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST THE MODEL\n",
    "\n",
    "-> if model gives good accuracy -> go to production\n",
    "\n",
    "-> if model gives not good accuracy -> will go for cycle this method\n",
    "\n",
    "will check after 15 days if model predicting good or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF WANT TO CHANGE NAME OF COLUMNS\n",
    "\n",
    "-> FILENAME=pd.read_csv('FILE NAME WHICH IS UPLOADED',names=range(0,61),header=0)\n",
    "\n",
    "-> in columns we can fill 0 which is start of columns and 61 which is your last column number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if inside the data there is any value is too large from minimum value to maximum value that means thare some soutlayers\n",
    "are present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CHECK OUTLAYERS OR CREATE SUMMRY/DESCRIBE OF DATA\n",
    "\n",
    "->    FILENAME.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO SHOW OUTLAYERS WE NEED TO BOXPLOT COLUMNS TO COLUMNS\n",
    "\n",
    "FOR SINGLE COLUMN\n",
    "\n",
    "-> FILENAME['COLUMNSNAME'].plot.box()\n",
    "\n",
    "FOR ALL COLUMNS LAYOUT MEANS SIZE OF BOXES\n",
    "\n",
    "->  FILENAME.plot(kind='box',subplots=True, layout=( 2,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO REMOVE OUTLAYERS WE NEED TO CALL ZSCORE FROM SCIPY LIBRARY\n",
    "\n",
    "\n",
    "\n",
    "-> from scipy.stats import zscore\n",
    "\n",
    "-> z=np.abs(zscore(COLUMNNAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKEWNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if inside the data there are some skewness present then we have to diskewed that by log function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO SHOW SKEWNESS INSIDE THE DATA WE NEED TO CREATE HISTOGRAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> FILENAME['COLUMNNAME'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO REMOVE SKEWNESS FROM DATA WE NEED TO CALL LOG FUNCTION FROM SCIPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "-> FILENAME['COLUMNNAME']=boxcox(FILENAME['COLUMNNAME'],0)\n",
    "\n",
    "0 -> Log transform\n",
    "\n",
    ".5 -> square root transform\n",
    "\n",
    "-> FILENAME['COLUMNNAME']=np.Log(FILENAME['COLUMNNAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELETIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if inside the data we have check that if any acoumns values are too negetive correleted with the target then we have\n",
    "to remove that entire column becouse it will harm to predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CHECK PARTICULAR COLUMN WITH TARGET WE NEED TO PLOT SCATTER THAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> plt.scatter(FILENAME['TARGETNAME'],FILENAME['COLUMNSNAME WHICH IS NEGETIVELLY CORRELATED WITH TARGET'])\n",
    "\n",
    "-> plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE CAN DROP SUCH COLUMNS\n",
    "\n",
    "-> FILENAME.drop('COLUMNNAME',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA --> Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if many columns are in dataset and it is too dificult to fit all in train and test set values\n",
    "then we will treat pca\n",
    "\n",
    "-> x = FILENAME.iloc[:,0:-1]\n",
    "\n",
    "-> pca = PCA(n_components=10)\n",
    "\n",
    "-> y = FILENAME.iloc[:,-1]\n",
    "\n",
    "-> xpca=pca.fit_transform(x)\n",
    "\n",
    "-> x=xpca\n",
    "\n",
    "-> pd.DataFrame(data=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STANDARD SCALER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFTER TREATING SKEWNESS, OUTLAYERS AND PCA WE NEED TO SCALE YOUR DATA TO GET GOOD POSITIVE RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "-> scale = StandardScaler()\n",
    "\n",
    "-> x = scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEAN-MAX SCALER  --> Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whenever we are having very high diffrence beetween minimu and maximum value into the dataset\n",
    "we need to do Normalized such data through mean-max scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE MODEL --> PICKLE AND JOBLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the trained model as a pickle string.\n",
    "\n",
    "saved_model=pickle.dumps(knn)  knn is model name\n",
    "\n",
    "# Load the pickled model\n",
    "\n",
    "knn_from_pickle=pickle.loads(saved_model)  knn is model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# save the model as a pickle in a file\n",
    "\n",
    "joblib.dump(knn,'filename.pkl')\n",
    "\n",
    "# load the model from the file\n",
    "\n",
    "knn_from_joblib=joblib.load('filename.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## LABEL ENCODER AND ONEHOT ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to change our data oject to numerical then we will use label encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> import sklearn\n",
    "\n",
    "-> from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "-> le=LabelEncoder()\n",
    "-> ds['COLUMNNAME']=le.fit_transform(ds['COLUMNSNAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREAT THE NULL VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> FILENAME['COUMNNAME']=FILENAME['COLUMNNAME'].fillna((FILENAME['COLUMNNAME'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->  import numpy as np\n",
    "\n",
    "->  FILENAME=FILENAME.replace(np.NaN,FILENAME['COLUMNNAME'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RENAME THE COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Since column names are complex lets change the names of the columns\n",
    "\n",
    "\n",
    "FILENAME.columns=[\"Cement\",\"Blast Furnace Slag\",\"Fly Ash\",\"Water\",\"Superplasticizer\",\"Coarse Aggregate\",\"Fine Aggregate\",\"Age\",\"Concrete compressive strength\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to create countplot in class value distribution\n",
    "\n",
    "\n",
    "sns.countplot(x=\"Class\",data=wine)\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
